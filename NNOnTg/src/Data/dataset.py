import tensorflow as tf
import json
from keras._tf_keras.keras.preprocessing.sequence import pad_sequences
from collections import OrderedDict, Counter, defaultdict
import numpy as np
import random


class Dataset:
    # –û—Ç–∫—Ä—ã–µ—Ç–∏–µ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ñ–∏–≥–æ–≤ –¥–ª—è –ø—É—Ç–µ–π
    with open("./config/PathConfig.json", 'r', encoding='utf-8') as paths_file:
        paths_file = json.load(paths_file) # –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—É—Ç–∏ 
        path_to_data_config = paths_file["DataConfig"] # –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        path_to_input_config = paths_file["InputConfig"] # –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É –≤—Ö–æ–¥–∞
        path_to_folder_with_datasets = paths_file["SaveDatasetPath"] # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        del paths_file # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    
    # –û—Ç–∫—Ä—ã–µ—Ç–∏–µ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ñ–∏–≥–æ–≤ –¥–ª—è –≤—Ö–æ–¥–∞
    with open(path_to_input_config, 'r', encoding='utf-8') as config_input_file: 
        input_config = json.load(config_input_file) # –ö–æ–Ω—Ñ–∏–≥ –¥–ª—è –≤—Ö–æ–¥–∞
        seq_length = input_config["seq_length"] # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        del input_config # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    # –û—Ç–∫—Ä—ã—Ç–∏–µ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ñ–∏–≥–æ–º –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
    with open(path_to_data_config, 'r', encoding='utf-8') as config_data_file:
        data_config = json.load(config_data_file) # –ö–æ–Ω—Ñ–∏–≥ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
        # path_to_file_with_row_tokenized_data = data_config["path_to_tokenized_file"] # –ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ —Å —Å—ã—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        path_to_file_with_row_tokenized_dialogs = data_config["path_to_tokenized_dialogs_file"] # –ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ —Å —Å—ã—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        path_to_file_with_row_tokenized_texts = data_config["path_to_tokenized_texts_file"] # –ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ —Å —Å—ã—Ä—ã–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        vocab_size = data_config["vocab_size"]
        dataset_config = data_config["dataset_config"] # –ö–æ–Ω—Ñ–∏–≥ –¥–∞—Ç–∞—Å–µ—Ç–∞
        path_to_dataset_json = dataset_config["path_to_dataset_json"] # –ü—É—Ç—å –¥–æ —Ñ–∞–π–ª–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ json —Ñ–æ—Ä–º–∞—Ç–µ
        shuffle_buffer_size = dataset_config["shuffle_buffer_size"]
        batch_size = dataset_config["batch_size"]
        del data_config # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π

    def __init__(self, skip_frequent_tokens=False, word_counts=None, is_pretrained=False,
                 filter_min_freq=10, filter_max_freq=1000, 
                 filter_rare_prob=0.1, filter_freq_prob=0.3):
        self.is_pretrained = is_pretrained
        if is_pretrained:
            self.raw_data = self.open_file(self.path_to_file_with_row_tokenized_texts)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –ø–æ –ø—É—Ç–∏ **{self.path_to_file_with_row_tokenized_texts}**")
        else:
            self.raw_data = self.open_file(self.path_to_file_with_row_tokenized_dialogs)
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –ø–æ –ø—É—Ç–∏ **{self.path_to_file_with_row_tokenized_dialogs}**")
        self.learneble_data = self.get_learnable_dataset(skip_frequent_tokens, word_counts)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if (filter_min_freq is not None and filter_max_freq is not None) and skip_frequent_tokens:
            # self.filter_dataset_by_frequency(
            #     min_freq=1,
            #     max_freq=20,
            #     rare_token_prob=filter_rare_prob,
            #     frequent_token_prob=filter_freq_prob
            # )
            # self.anti_spam_filter(self.learneble_data[0], self.learneble_data[1])
            # self.diasable_eos_id(self.learneble_data[0], self.learneble_data[1])
            self.diasable_neuron_id(self.learneble_data[0], self.learneble_data[1], 1)
            self.diasable_neuron_id(self.learneble_data[0], self.learneble_data[1], 6)
            self.balance_frequent_tokens(self.learneble_data[0], self.learneble_data[1])
            # self.anti_spam_filter(self.learneble_data[0], self.learneble_data[1])

    def diasable_neuron_id(self, X, Y, id=1):
        filtered_X = []
        filtered_Y = []
        removed_count = 0
        original_size = len(X)

        # eos_token_id = 6
        
        for x, y in zip(X, Y):
            if y == id:
                removed_count += 1
                continue
            filtered_X.append(x)
            filtered_Y.append(y)
        
        new_size = len(filtered_X)
        removed_count = original_size - new_size
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ç–∞—Ä–≥–µ—Ç–æ–≤: {removed_count}")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–æ–≤: {len(filtered_X)}")

        self.learneble_data = (filtered_X, filtered_Y)
        return filtered_X, filtered_Y
        

    def balance_frequent_tokens(self, X, Y, max_examples_per_token=100, max_frequency=100):
        """
        –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç: –¥–ª—è —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ max_examples_per_token –ø—Ä–∏–º–µ—Ä–æ–≤
        """
        # special_tokens={'<eos>', "–ø—Ä–∏–≤–µ—Ç"}
        special_tokens={}
        # from collections import Counter, 
        
        # –°—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        removed_count = 0
        token_counts = Counter(Y)
        
        print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {len(token_counts)}")
        original_size = len(X)
        
        
        filtered_X, filtered_Y = [], []
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø–æ —Ü–µ–ª–µ–≤—ã–º —Ç–æ–∫–µ–Ω–∞–º
        token_to_examples = defaultdict(list)
        for x, y in zip(X, Y):
            token_to_examples[y].append((x, y))
        
        balanced_tokens = set()
        
        for token, examples in token_to_examples.items():
            # –î–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ - –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ
            # if token in special_tokens:
            #     filtered_X.extend([ex[0] for ex in examples])
            #     filtered_Y.extend([ex[1] for ex in examples])
            #     continue
                
            # –ï—Å–ª–∏ —Ç–æ–∫–µ–Ω –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —Ä–µ–¥–∫–æ - –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ
            if len(examples) <= max_frequency:
                filtered_X.extend([ex[0] for ex in examples])
                filtered_Y.extend([ex[1] for ex in examples])
            else:
                # –î–ª—è —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ max_examples_per_token —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
                selected_examples = random.sample(examples, min(max_examples_per_token, len(examples)))
                filtered_X.extend([ex[0] for ex in selected_examples])
                filtered_Y.extend([ex[1] for ex in selected_examples])
                balanced_tokens.add(token)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        # return filtered_X, filtered_Y
        new_size = len(filtered_X)
        removed_count = original_size - new_size

        
        print(f"üîß –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –î–ê–ù–ù–´–•:")
        print(f"–ë—ã–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {original_size}")
        print(f"–°—Ç–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {new_size}")
        print(f"–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {removed_count}")
        print(f"–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(balanced_tokens)}")
        print(f"–¢–æ–ø-10 —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: {list(balanced_tokens)[:10]}")
        
        self.learneble_data = (filtered_X, filtered_Y)
        return filtered_X, filtered_Y

    def anti_spam_filter(self, X, Y, min_diversity=0.4):
        """
        –ñ–µ—Å—Ç–∫–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ —Å–ø–∞–º–∞ —á–∞—Å—Ç—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏
        """
        # import numpy as np
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ü–µ–ª–µ–≤—ã–º —Ç–æ–∫–µ–Ω–∞–º
        token_counts = Counter(Y)
        # total_samples = len(Y)
        
        # –¢–æ–ø-N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ø–∞–º-–∫–∞–Ω–¥–∏–¥–∞—Ç—ã)
        top_tokens = [token for token, count in token_counts.most_common()]
        
        filtered_X, filtered_Y = [], []
        spam_count = 0
        
        for x, y in zip(X, Y):
            # –ü—Ä–∞–≤–∏–ª–æ 1: –£–¥–∞–ª–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≥–¥–µ —Ü–µ–ª–µ–≤–æ–π —Ç–æ–∫–µ–Ω –≤ —Ç–æ–ø-20 —á–∞—Å—Ç—ã—Ö
            if y in top_tokens[:20]:
                spam_count += 1
                continue
                
            # –ü—Ä–∞–≤–∏–ª–æ 2: –£–¥–∞–ª–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã —Å –Ω–∏–∑–∫–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            unique_ratio = len(set(x)) / len(x) if len(x) > 0 else 0
            if unique_ratio < min_diversity:
                spam_count += 1
                continue
                
            # –ü—Ä–∞–≤–∏–ª–æ 3: –£–¥–∞–ª–∏—Ç—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if len(x) < 4:
                spam_count += 1
                continue
                
            filtered_X.append(x)
            filtered_Y.append(y)
        
        print(f"–£–¥–∞–ª–µ–Ω–æ —Å–ø–∞–º-–ø—Ä–∏–º–µ—Ä–æ–≤: {spam_count}")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–æ–≤: {len(filtered_X)}")
        self.learneble_data = (filtered_X, filtered_Y)

        return filtered_X, filtered_Y


    def filter_dataset_by_frequency(self, min_freq=2, max_freq=1000, 
                                  rare_token_prob=0.2, frequent_token_prob=0.4,
                                  keep_special_tokens=True):
        """
        –ö–û–†–†–ï–ö–¢–ù–ê–Ø —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ —Ç–æ–∫–µ–Ω–æ–≤
        """
        X, Y = self.learneble_data
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –í–°–ï–ú —Ç–æ–∫–µ–Ω–∞–º –≤ —Ü–µ–ª—è—Ö (Y)
        token_counts = Counter(Y)
        print(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ü–µ–ª—è—Ö –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(token_counts)}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        special_tokens = {'<unk>', '<pad>', '<eos>', '<sos>', '<mask>', '–ø—Ä–∏–≤–µ—Ç', ',', '.', '!', '?'}
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–ª—è –ö–ê–ñ–î–û–ì–û —Ü–µ–ª–µ–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        filtering_rules = {}
        for token, count in token_counts.items():
            if token in special_tokens and keep_special_tokens:
                filtering_rules[token] = {'action': 'keep', 'prob': 1.0}
            elif count < min_freq:
                filtering_rules[token] = {'action': 'remove', 'prob': 0.0}
            elif count < min_freq * 3:  # –£–≤–µ–ª–∏—á–∏–ª –¥–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–¥–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
                filtering_rules[token] = {'action': 'keep_prob', 'prob': rare_token_prob}
            elif count > max_freq:
                filtering_rules[token] = {'action': 'remove_prob', 'prob': frequent_token_prob}
            else:
                filtering_rules[token] = {'action': 'keep', 'prob': 1.0}
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
        filtered_X = []
        filtered_Y = []
        removed_count = 0
        
        for i, (input_seq, target) in enumerate(zip(X, Y)):
            rule = filtering_rules.get(target, {'action': 'keep', 'prob': 1.0})
            
            should_keep = False
            if rule['action'] == 'keep':
                should_keep = True
            elif rule['action'] == 'keep_prob':
                should_keep = np.random.random() < rule['prob']
            elif rule['action'] == 'remove_prob':
                should_keep = np.random.random() > rule['prob']
            # 'remove' action - should_keep –æ—Å—Ç–∞–µ—Ç—Å—è False
            
            if should_keep:
                filtered_X.append(input_seq)
                filtered_Y.append(target)
            else:
                removed_count += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.learneble_data = (filtered_X, filtered_Y)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"–£–¥–∞–ª–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {removed_count}")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–∏–º–µ—Ä–æ–≤: {len(filtered_X)}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ü–µ–ª—è—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(set(filtered_Y))}")
        
        return self.learneble_data
    
    @classmethod
    def open_file(self, path):
        """ –ú–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–∫—Ä—ã—Ç–∏—è —Ñ–∞–π–ª–∞ —Å —Å—ã—Ä–º–∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ """
        with open(path, 'r', encoding='utf-8') as file_with_row_data:
            row_data = json.load(file_with_row_data) # –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            return row_data
    
    def prepare_dataset_with_padding(self, data, max_length=None, pad_token_id=0):
        """ –ú–µ—Ç–æ–¥ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ """
        inputs, targets = data # –†–∞–∑–ª–æ–∂–µ–Ω–∏–µ –≤—Å–µ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –¥–∞–Ω–Ω—ã–º–∏ –Ω–∞ –≤—Ö–æ–¥ –∏ —Ç–∞—Ä–≥–µ—Ç
        mli = max(inputs, key=len)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ, –µ—Å—Ç—å –ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –≤—Ö–æ–¥ —Å–µ—Ç–∏. –ü–æ —Å—É—Ç–∏, —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ï—Å–ª–∏ –Ω–µ—Ç, —Ç–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –±—É–¥–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if max_length is None: 
            max_length = len(max(inputs, key=len))
        # –ü–∞–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        inputs_padded = pad_sequences(
            inputs, # –ò–∑–Ω–∞—á–∞–ª—å–Ω—ã–µ –≤—Ö–æ–¥—ã 
            maxlen=max_length, # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            dtype='int32', # –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (–∏–Ω—Ç –Ω–∞ 4 –±–∞–π—Ç–∞)
            padding='pre', # –î–æ–±–∞–≤–ª—è—Ç—å <PAD> –≤ –∫–æ–Ω–µ—Ü
            truncating='pre', #  –û–±—Ä–µ–∑–∞—Ç—å –¥–ª–∏–Ω—É c –Ω–∞—á–∞–ª–∞
            value=pad_token_id # –ê–π–¥–∏ –ø–∞–¥-—Ç–æ–∫–µ–Ω–∞
        )
        return inputs_padded, targets
    
    def create_tf_dataset_fix_length(self, learneble_padding_data, shuffle=True):
        """ –ú–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Å–∞ tf.data.Dataset
        learneble_padding_data - –ø–∞–¥–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        shuffle - (True/False) –µ—Å–ª–∏ –¥–∞, —Ç–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç. –ó–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –±—É—Ñ—Ñ–µ—Ä–∞ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è –±–µ—Ä–µ—Ç—Å—è –∏–∑ —Ñ–∞–π–ª–∞ —Å –∫–æ–Ω—Ñ–∏–≥–æ–º
        """
        inputs_padded, targets = learneble_padding_data # –†–∞–∑–±–æ—Ä –Ω–∞ –≤—Ö–æ–¥—ã –∏ —Ç–∞—Ä–≥–µ—Ç—ã 
        inputs_tensor = tf.constant(inputs_padded, dtype=tf.int32) # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        targets_tensor = tf.constant(targets, dtype=tf.int32) # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        self.learneble_data_tf = tf.data.Dataset.from_tensor_slices((inputs_tensor, targets_tensor)) # –û–±—É—á–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–∏–ø–∞ –∏–∑ TensorFlow
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ shuffle = True
        if shuffle: 
            self.learneble_data_tf = self.learneble_data_tf.shuffle(
                buffer_size=self.shuffle_buffer_size, # –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è
                reshuffle_each_iteration=True # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            )
        self.learneble_data_tf = self.learneble_data_tf.batch(batch_size=self.batch_size)
        

    def create_tf_dataset(self, learneble_data, shuffle=True):
        inputs, targets = learneble_data # –†–∞–∑–±–æ—Ä –Ω–∞ –≤—Ö–æ–¥—ã –∏ —Ç–∞—Ä–≥–µ—Ç—ã 
        inputs_tensor = tf.ragged.constant(inputs, dtype=tf.int32) # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        targets_tensor = tf.ragged.constant(targets, dtype=tf.int32) # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–æ–≤ –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        self.learneble_data_tf = tf.data.Dataset.from_tensor_slices((inputs_tensor, targets_tensor)) # –û–±—É—á–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ç–∏–ø–∞ –∏–∑ TensorFlow
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ shuffle = True
        if shuffle: 
            self.learneble_data_tf = self.learneble_data_tf.shuffle(
                buffer_size=self.shuffle_buffer_size, # –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è
                reshuffle_each_iteration=True # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞—Ç—å –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            )
        self.learneble_data_tf = self.learneble_data_tf.map(
            lambda x, y: (
                tf.cast(x, dtype=tf.int32),
                tf.cast(y, dtype=tf.int32)
                # tf.cast(tf.convert_to_tensor(x, dtype=tf.int32), tf.int32),
                # tf.cast(tf.convert_to_tensor(y, dtype=tf.int32), tf.int32)
            ))
        
        def reverse_sequence(x, y):
            return tf.reverse(x, axis=[0]), y
            
        # –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.learneble_data_tf = self.learneble_data_tf.map(reverse_sequence)
        self.learneble_data_tf = self.learneble_data_tf.padded_batch(
            self.batch_size,
            padded_shapes=(
                tf.TensorShape([50]), []),
            padding_values=(0, 0),
            drop_remainder=True
        ).prefetch(tf.data.AUTOTUNE)

        def reverse_batch(x_batch, y_batch):
            return tf.reverse(x_batch, axis=[1]), y_batch
    
        self.learneble_data_tf = self.learneble_data_tf.map(reverse_batch)

    def create_batches(self, batch_size=32):
        """ –ú–µ—Ç–æ–¥ –¥–ª—è —Å–±–æ—Ä–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±—ç—Ç—á–∏. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ —á—Ç–æ–±—ã —Å–Ω–∞—á–∞–ª–∞ –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–µ—Ç–æ–¥ create_tf_dataset"""
        self.learneble_data_tf = self.learneble_data_tf.batch(batch_size=batch_size)

    def get_learnable_dataset(self, ignore_frequent_tokens=False, word_counts: OrderedDict=None):
        """ –ú–µ—Ç–æ–¥ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å—ã—Ä—ã—Ö —Ç–æ–∫–µ–Ω–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        Args:
            ignore_frequent_tokens: bool - –§–ª–∞–≥, —É–∫–∞–∑—ã–≤–∞—é—â–∏–π –Ω–∞ —Ç–æ, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–ª–∏ –Ω–µ—Ç
            is_pre_train_data: bool - 
            word_counts: OrderesDict - 
        """
        X = []  # –í—Ö–æ–¥—ã
        Y = []  # –í—ã—Ö–æ–¥—ã (—Ü–µ–ª–∏)
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if self.is_pretrained: # –ü—Ä–µ—Ç—Ä–µ–∏–Ω–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏–µ
            # –ü–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            for text in self.raw_data: 
                text = text["text"] # –¢–µ–∫—É—â–∏–π —Ç–µ–∫—Å—Ç
                current_sequence = [] # –¢–µ–∫—É—â–∞—è –¥–ª–∏–Ω–∞. –ù—É–∂–Ω–æ —á—Ç–æ–±—ã –¥–æ–±–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ "–ª–µ—Å–µ–Ω–∫–æ–π"

                for token in text:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ, –µ—Å—Ç—å –ª–∏ —Å–µ–π—á–∞—Å –∫–∞–∫–∏–µ-—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
                    if len(current_sequence) < 4 - 1: # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—É—Å—Ç–∞—è
                    # if len(current_sequence) < self.seq_length - 1: # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—É—Å—Ç–∞—è
                        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –æ–±—â—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                        current_sequence.append(token) 
                    else: # –ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–∞ –æ–∫–Ω–∞
                    # else: # –ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ –ø—É—Å—Ç–∞—è
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π
                        if len(current_sequence) == self.seq_length: # –ï—Å–ª–∏ —è–≤–ª—è–µ—Ç—Å—è - —Ç–æ —É–¥–∞–ª—è—é—Ç—Å—è –ø–µ—Ä–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
                            current_sequence.pop(0) 
                        # –í—Ö–æ–¥: —Ç–µ–∫—É—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                        X.append(current_sequence.copy())
                        # –í—ã—Ö–æ–¥: —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
                        Y.append(token)
                        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        current_sequence.append(token)
        else:
            # –ü–µ—Ä–µ–±–æ—Ä –≤—Å–µ—Ö –¥–∏–∞–ª–æ–≥–æ–≤
            for dialog in self.raw_data:
                dialog = dialog['dialog'] # –¢–µ–∫—É—â–∏–π –¥–∏–∞–ª–æ–≥
                current_sequence = [] # –¢–µ–∫—É—â–∞—è –¥–ª–∏–Ω–∞. –ù—É–∂–Ω–æ —á—Ç–æ–±—ã –¥–æ–±–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ "–ª–µ—Å–µ–Ω–∫–æ–π"
                
                for j, message in enumerate(dialog):
                # for j, message in enumerate(dialog[:2]):
                    if j % 2 == 0:  
                        # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ–π —Ñ—Ä–∞–∑—ã
                        current_sequence.extend(message)
                    else:  
                        # –ë–æ—Ç - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
                        for token in message:
                        # for token in message[:4]:
                            # if not (ignore_frequent_tokens and (word_counts[token] <= 1500) and (word_counts[token] >= 200)and token != 1):
                            # –í—Ö–æ–¥: —Ç–µ–∫—É—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                            X.append(current_sequence.copy())
                            # –í—ã—Ö–æ–¥: —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
                            Y.append(token)
                            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                            current_sequence.append(token)
            
        return X, Y
    
    
    def prepare_dataset_to_tf(self, shuffle, fix_length=False):
        """ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫ –æ–±—É—á–µ–Ω–∏—é —Å–µ—Ç–∏ (json —Å—Ç–∏–∞–Ω–æ–≤–∏—Ç—Å—è —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ tensorflow). 
        shuffle - —Ä–∞–∑–º–µ—Ä –¥–ª—è –±—É—Ñ–µ—Ä–∞ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è """
        if fix_length:
            self.learnable_padding_data = self.prepare_dataset_with_padding(self.learneble_data, self.seq_length)
            self.create_tf_dataset_fix_length(self.learnable_padding_data)
        else:
            self.create_tf_dataset(self.learneble_data, shuffle) # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤ –≤–∏–¥–µ tf
            # self.create_tf_dataset(self.learneble_data, shuffle) # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤ –≤–∏–¥–µ tf

        # self.create_tf_dataset(self.learneble_padding_data, shuffle) # –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏–∏—è —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤ –≤–∏–¥–µ tf
        # self.create_batches(self.batch_size) # –°–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ –±—ç—Ç—á–∏
        
        return self.learneble_data_tf

    def save_dataset_to_json(self):
        """ –ú–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ self.learneble_data –≤ —Ñ–∞–π–ª dataset.json """
        with open(self.path_to_dataset_json, 'w', encoding='utf-8') as dataset_json_file:
            json.dump(self.learneble_data, dataset_json_file, indent=4)


if __name__ == '__main__':
    dt = Dataset(is_pretrained=True)
    print(dt.learneble_data[0])
